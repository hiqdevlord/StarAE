Optimization Project
====

##### Note that this folder is built for the Numerical Optimization course final project, instructed by Professor Margaret Wright in Courant Institute, New York Univeristy.

Introduction
---
The project will mostly focus on the training optimization for Autoencoder, which is a simple and widely used Neural Nets model. I will propose a series of results about optimizing Autoencoder in this two weeks (rather time-intensive).

Targets
---
SGD vs BFGS vs CG

Different settings of SGD, including Momentum, Adagrad and Mini-batch

Scale of training set

Numerical issues exploration

Results
---
cmp_S means Scale comparisons
cmp_M means Momentum comparisons
cmp_Mb means Mini-batch comparisons
cmp_N_sgd means Numerical issue investigation for SGD
cmp_N_bfgs means Numerical issue investigation for BFGS
cmp_N_cg means Numerical issue investigation for CG

Acknowledgement
---
I would like to thank Prof. Margaret Wright!


